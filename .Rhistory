p_value = pval,
significant = TRUE,
true_d = true_d,
power = pwr_result,
observed_d = observed_d,
BUCSS = adjusted_bucss_d,
puniform = puniform_d,
type_m = adjusted_prda_d
))
}
#| label: fig-correction-comparison
#| fig-cap: "Comparison of the corrected effect sizes for observed Cohen's d effect sizes from 0 to 1 using BUCSS, p-uniform, and Type M errors."
#| warning: false
#| message: false
#| echo: false
#| fig-align: center
# Loop over mu1 from 0 to 1 in steps of 0.1
mu1_values <- seq(0, 1, by = 0.01)
all_results <- lapply(mu1_values, function(m1) {
res <- compare_effect_size_corrections(
n = 20,
mu1 = m1,
mu2 = 0,
sd = 1,
sided = 2,
exact = TRUE,
retrodesign_d = NULL)
res$mu1 <- m1
res
})
library(effsize)   # for Cohen's d
# Custom function to compare effect size correction approaches
compare_effect_size_corrections <- function(n = 20, mu1 = 1, mu2 = 0, sd = 1, alpha = 0.05, retrodesign_d = NULL, sided = 1, exact = TRUE) {
# ---- Step 1: Simulate data ----
if (exact) {
group1 <- MASS::mvrnorm(
n = n,
mu = mu1,
Sigma = matrix(sd^2, 1),
empirical = TRUE)
group2 <- MASS::mvrnorm(
n = n,
mu = mu2,
Sigma = matrix(sd^2, 1),
empirical = TRUE)
} else {
group1 <- rnorm(n, mu1, sd)
group2 <- rnorm(n, mu2, sd)
}
# ---- Step 2: Compute Cohen's d ----
cohen_result <- cohen.d(group1, group2, pooled = TRUE)
observed_d <- as.numeric(cohen_result$estimate)
true_d <- ((mu1 - mu2) / sd)
# Compute power for one-sided two-sample t-test
alternative_type <- if (sided == 1) "greater" else "two.sided"
pwr_result <- pwr.t.test(
d = true_d,
n = n,
sig.level = alpha,
type = "two.sample",
alternative = alternative_type
)$power
# ---- Step 3: Compute SE of d ----
compute_se_d <- function(n_per_group) sqrt(2 / n_per_group)
se_d <- compute_se_d(n)
# ---- Step 4: Perform t-test ----
t_test_result <- t.test(group1, group2, var.equal = TRUE)
tobs <- as.numeric(t_test_result$statistic)
df <- 2 * n - 2
pval <- t_test_result$p.value
# ---- Step 6: BUCSS ----
adjusted_bucss_d <- tryCatch({
suppressWarnings({
BUCSS_result <- ss.power.it(
t.observed = tobs,
n = n,
alpha.prior = alpha,
alpha.planned = alpha,
assurance = 0.50,
power = 0.80,
step = 0.001
)
BUCSS_result[[2]] / sqrt(n / 2)
})
}, error = function(e) {
NA
})
# ---- Step 7: puniform correction ----
puniform_d <- tryCatch({
result <- puniform(
n1i = n,
n2i = n,
tobs = tobs,
side = "right",
plot = FALSE
)
result$est
}, error = function(e) {
return(NA)
})
# ---- Step 8: Type M error PRDA package ----
alternative_type <- if (sided == 1) "greater" else "two_sided"
if (is.null(retrodesign_d)) {
retrodesign_d <- observed_d
}
prda <- PRDA::retrospective(
effect_size = retrodesign_d,
sample_n1 = n,
sample_n2 = n,
test_method = "two_sample",
alternative = alternative_type
)
typeM_prda <- prda$retrospective_res$typeM
# Even though Type M errors are not intended as corrections, it is misused this way in the literature. We aim to illustrate why this is not correct.
adjusted_prda_d <- observed_d / typeM_prda
invisible(list(
p_value = pval,
significant = TRUE,
true_d = true_d,
power = pwr_result,
observed_d = observed_d,
BUCSS = adjusted_bucss_d,
puniform = puniform_d,
type_m = adjusted_prda_d
))
}
library(effsize)   # for Cohen's d
library(weightr)   # for selection model bias correction
library(puniform)  # for p-curve bias correction
library(BUCSS)     # for BUCSS bias correction
library(PRDA)      # for Type S and M error calculation
library(pwr)
library(dplyr)
library(tidyverse)
library(ggplot2)
# remotes::install_github("psicostat/criticalESvalue")
library(criticalESvalue)  # for critical effect size calculation
library(progress)
library(forcats)
# Custom function to compare effect size correction approaches
compare_effect_size_corrections <- function(n = 20, mu1 = 1, mu2 = 0, sd = 1, alpha = 0.05, retrodesign_d = NULL, sided = 1, exact = TRUE) {
# ---- Step 1: Simulate data ----
if (exact) {
group1 <- MASS::mvrnorm(
n = n,
mu = mu1,
Sigma = matrix(sd^2, 1),
empirical = TRUE)
group2 <- MASS::mvrnorm(
n = n,
mu = mu2,
Sigma = matrix(sd^2, 1),
empirical = TRUE)
} else {
group1 <- rnorm(n, mu1, sd)
group2 <- rnorm(n, mu2, sd)
}
# ---- Step 2: Compute Cohen's d ----
cohen_result <- cohen.d(group1, group2, pooled = TRUE)
observed_d <- as.numeric(cohen_result$estimate)
true_d <- ((mu1 - mu2) / sd)
# Compute power for one-sided two-sample t-test
alternative_type <- if (sided == 1) "greater" else "two.sided"
pwr_result <- pwr.t.test(
d = true_d,
n = n,
sig.level = alpha,
type = "two.sample",
alternative = alternative_type
)$power
# ---- Step 3: Compute SE of d ----
compute_se_d <- function(n_per_group) sqrt(2 / n_per_group)
se_d <- compute_se_d(n)
# ---- Step 4: Perform t-test ----
t_test_result <- t.test(group1, group2, var.equal = TRUE)
tobs <- as.numeric(t_test_result$statistic)
df <- 2 * n - 2
pval <- t_test_result$p.value
# ---- Step 6: BUCSS ----
adjusted_bucss_d <- tryCatch({
suppressWarnings({
BUCSS_result <- ss.power.it(
t.observed = tobs,
n = n,
alpha.prior = alpha,
alpha.planned = alpha,
assurance = 0.50,
power = 0.80,
step = 0.001
)
BUCSS_result[[2]] / sqrt(n / 2)
})
}, error = function(e) {
NA
})
# ---- Step 7: puniform correction ----
puniform_d <- tryCatch({
result <- puniform(
n1i = n,
n2i = n,
tobs = tobs,
side = "right",
plot = FALSE
)
result$est
}, error = function(e) {
return(NA)
})
# ---- Step 8: Type M error PRDA package ----
alternative_type <- if (sided == 1) "greater" else "two_sided"
if (is.null(retrodesign_d)) {
retrodesign_d <- observed_d
}
prda <- PRDA::retrospective(
effect_size = retrodesign_d,
sample_n1 = n,
sample_n2 = n,
test_method = "two_sample",
alternative = alternative_type
)
typeM_prda <- prda$retrospective_res$typeM
# Even though Type M errors are not intended as corrections, it is misused this way in the literature. We aim to illustrate why this is not correct.
adjusted_prda_d <- observed_d / typeM_prda
invisible(list(
p_value = pval,
significant = TRUE,
true_d = true_d,
power = pwr_result,
observed_d = observed_d,
BUCSS = adjusted_bucss_d,
puniform = puniform_d,
type_m = adjusted_prda_d
))
}
#| label: fig-correction-comparison
#| fig-cap: "Comparison of the corrected effect sizes for observed Cohen's d effect sizes from 0 to 1 using BUCSS, p-uniform, and Type M errors."
#| warning: false
#| message: false
#| echo: false
#| fig-align: center
# Loop over mu1 from 0 to 1 in steps of 0.1
mu1_values <- seq(0, 1, by = 0.01)
all_results <- lapply(mu1_values, function(m1) {
res <- compare_effect_size_corrections(
n = 20,
mu1 = m1,
mu2 = 0,
sd = 1,
sided = 2,
exact = TRUE,
retrodesign_d = NULL)
res$mu1 <- m1
res
})
# Combine all into one table
final_table <- bind_rows(all_results) %>%
select(mu1, everything())
# Combine all into one table
final_table <- bind_rows(all_results) %>%
select(mu1, everything())
# Reshape the data to long format
plot_data <- final_table %>%
filter(significant) %>%  # Only include significant results
pivot_longer(
cols = c(power, BUCSS, puniform, type_m),
names_to = "Method",
values_to = "Adjusted_d"
)
# Recode method labels for consistency
plot_data <- plot_data %>%
mutate(Method = recode(Method,
"puniform" = "P-uniform",
"type_m" = "Type M",
"BUCSS" = "BUCSS",
"power" = "Power"))
# Set factor levels to control legend and line order
plot_data$Method <- factor(plot_data$Method,
levels = c("Power", "Type M", "P-uniform", "BUCSS"))
# Plot
ggplot(plot_data, aes(x = observed_d, y = Adjusted_d, color = Method, linetype = Method)) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray80", linewidth = 1) +
geom_line(linewidth = 1.7) +
scale_color_manual(values = c(
"P-uniform" = "#bb7125",
"Type M" = "#ffdd00",
"Power" = "#00978d",
"BUCSS" = "#1c4286"
)) +
scale_linetype_manual(values = c(
"Power" = "solid",
"Type M" = "solid",
"P-uniform" = "solid",
"BUCSS" = "dotted"
)) +
scale_x_continuous(limits = c(0, 1)) +
scale_y_continuous(limits = c(0, 1)) +
coord_fixed(ratio = 1) +
labs(
title = "Bias-Corrected Estimates vs. Observed d",
x = "Cohen's d",
y = "Corrected Cohen's d and Power",
color = NULL,
linetype = NULL
) +
theme_minimal(base_size = 14) +
labs(color = NULL, fill = NULL, linetype = NULL) +
theme(
legend.position = "bottom",
text = element_text(family = "Segoe UI")
)
#| label: fig-correction-comparison
#| fig-cap: "Comparison of the corrected effect sizes for observed Cohen's d effect sizes from 0 to 1 using BUCSS, p-uniform, and Type M errors."
#| warning: false
#| message: false
#| echo: false
#| fig-align: center
# Loop over mu1 from 0 to 1 in steps of 0.1
mu1_values <- seq(0, 1, by = 0.01)
all_results <- lapply(mu1_values, function(m1) {
res <- compare_effect_size_corrections(
n = 40,
mu1 = m1,
mu2 = 0,
sd = 1,
sided = 2,
exact = TRUE,
retrodesign_d = NULL)
res$mu1 <- m1
res
})
# Combine all into one table
final_table <- bind_rows(all_results) %>%
select(mu1, everything())
# Reshape the data to long format
plot_data <- final_table %>%
filter(significant) %>%  # Only include significant results
pivot_longer(
cols = c(power, BUCSS, puniform, type_m),
names_to = "Method",
values_to = "Adjusted_d"
)
# Recode method labels for consistency
plot_data <- plot_data %>%
mutate(Method = recode(Method,
"puniform" = "P-uniform",
"type_m" = "Type M",
"BUCSS" = "BUCSS",
"power" = "Power"))
# Set factor levels to control legend and line order
plot_data$Method <- factor(plot_data$Method,
levels = c("Power", "Type M", "P-uniform", "BUCSS"))
# Plot
ggplot(plot_data, aes(x = observed_d, y = Adjusted_d, color = Method, linetype = Method)) +
geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray80", linewidth = 1) +
geom_line(linewidth = 1.7) +
scale_color_manual(values = c(
"P-uniform" = "#bb7125",
"Type M" = "#ffdd00",
"Power" = "#00978d",
"BUCSS" = "#1c4286"
)) +
scale_linetype_manual(values = c(
"Power" = "solid",
"Type M" = "solid",
"P-uniform" = "solid",
"BUCSS" = "dotted"
)) +
scale_x_continuous(limits = c(0, 1)) +
scale_y_continuous(limits = c(0, 1)) +
coord_fixed(ratio = 1) +
labs(
title = "Bias-Corrected Estimates vs. Observed d",
x = "Cohen's d",
y = "Corrected Cohen's d and Power",
color = NULL,
linetype = NULL
) +
theme_minimal(base_size = 14) +
labs(color = NULL, fill = NULL, linetype = NULL) +
theme(
legend.position = "bottom",
text = element_text(family = "Segoe UI")
)
# Critical effect size
# critical value from the t statistic
m1<-0.5
m2<-0
sd1<-1
sd2<-1
n1<-40
n2<-40
se <- sqrt(sd1^2 / n1 + sd2^2 / n2)
t <- (m1 - m2) / se
crit_res <- critical_t2s(t = t, n1 = n1, n2 = n2, se = se)
crit_res$dc
#| label: fig-effectsize-distribution
#| fig-cap: "Expected effect size distribution under the alternative hypothesis, assuming a true effect size of 0.5 (dashed central line). With a sample size of 20 participants per group, a two-sided independent t-test yields a statistically significant result when the observed effect size exceeds d = 0.64. The shaded area represents the subset of effect sizes published under selection bias."
#| warning: false
#| message: false
#| echo: false
#| fig-align: center
## Figure illustrating critical effect size
low_x <- -1
high_x <- 2
y_max <- 2
# Add Type 1 error rate function
add_type1_error <- function(N,
side = "right",
ncp,
col = "#00978d") {
mult <- ifelse(side == "right", 1, -1)
crit_d <- mult * abs(qt(0.05 / 2, (N * 2) - 2)) / sqrt(N / 2)
if (side == "right") {
y <- seq(crit_d, 10, length = 10000)
} else {
y <- seq(-10, crit_d, length = 10000)
}
# determine upperbounds polygon
suppressWarnings({
z <- (dt(y * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2))
})
if (side == "right") {
polygon(c(crit_d, y, 10), c(0, z, 0), col = col)
} else {
polygon(c(y, crit_d, crit_d), c(z, 0, 0), col = col)
}
}
# calculate distribution of d based on t-distribution
calc_d_dist <- function(x, N, ncp = 0) {
suppressWarnings({
# generates a lot of warnings sometimes
dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2)
})
}
# Set sample size per group and effect size d (assumes equal sample sizes per group)
N <- 40 # sample size per group for independent t-test
d <- 0.5 # please enter positive d only
# Calculate non-centrality parameter - equals t-value from sample
ncp <- d * sqrt(N / 2)
# calc d-distribution
x <- seq(low_x, high_x, length = 10000) # create x values
d_dist <- calc_d_dist(x, N, ncp)
# Set max Y
y_max <- max(d_dist) + 0.5
crit_d <- abs(qt(0.05 / 2, (N * 2) - 2)) / sqrt(N / 2)
# Create base plot with no x-axis drawn
plot(-10, xlim = c(low_x, high_x), ylim = c(0, y_max),
xlab = expression("Cohen's " * italic(d)),
ylab = "",
main = bquote(
paste("Distribution of effects sizes, N = 20, ",
italic(d), " = ", .(d), ", ",
d[crit], " = ", .(round(crit_d, 2)))
),
xaxt = "n",   # disable default x-axis
yaxt = "n"   # disable default x-axis
)
# Add major ticks at each 0.5
axis(side = 1, at = seq(low_x, high_x, by = 0.5), labels = TRUE, lwd = 1.5)
# Add minor ticks at each 0.1 (no labels)
axis(side = 1, at = seq(low_x, high_x, by = 0.1), labels = FALSE, tcl = -0.25)  # tcl controls tick length
d_dist <- dt(x * sqrt(N / 2), df = (N * 2) - 2, ncp = ncp) * sqrt(N / 2)
lines(x, d_dist, col = "black", type = "l", lwd = 2)
# Add type 1 error rate
add_type1_error(N, "right", ncp)
add_type1_error(N, "left", ncp)
abline(v=d, lty = 2)
crit_from_t_t2s(
t = 1,
n1 = 40,
n2 = 40,
se = 1,
conf.level,
hypothesis,
var.equal = FALSE
)
# remotes::install_github("psicostat/criticalESvalue")
library(criticalESvalue)  # for critical effect size calculation
crit_from_t_t2s(
t = 1,
n1 = 40,
n2 = 40,
se = 1,
conf.level,
hypothesis,
var.equal = FALSE
)
# critical value from the t statistic
m1<-0.5
m2<-0
sd1<-1
sd2<-1
n1<-40
n2<-40
se <- sqrt(sd1^2 / n1 + sd2^2 / n2)
t <- (m1 - m2) / se
crit_res <- critical_t2s(t = t, n1 = n1, n2 = n2, se = se)
crit_res <- critical_t2s(t = t, n1 = n1, n2 = n2, se = se, var.equal = TRUE)
crit_res$dc
# critical d value for all examples with n = 40
m1<-0.5
m2<-0
sd1<-1
sd2<-1
n1<-40
n2<-40
se <- sqrt(sd1^2 / n1 + sd2^2 / n2)
t <- (m1 - m2) / se
crit_res <- critical_t2s(t = t, n1 = n1, n2 = n2, se = se, var.equal = TRUE)
critical_d <- crit_res$dc
critical_d
round(critical_d, 3)
