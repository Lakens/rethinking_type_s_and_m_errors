<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Daniel Lakens">
<meta name="author" content="Cristian Mesquida">
<meta name="author" content="Gabriela Xavier-Quintais">
<meta name="author" content="Sajedeh Rasti">
<meta name="author" content="Enrico Toffalini">
<meta name="author" content="Gianmarco Altoè">
<meta name="keywords" content="Type M error, Type S error, Error Control, Bias Correction">
<meta name="description" content="RETHINKING TYPE S AND M ERRORS">

<title>Rethinking Type S and M Errors</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Type M error correction_files/libs/clipboard/clipboard.min.js"></script>
<script src="Type M error correction_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="Type M error correction_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="Type M error correction_files/libs/quarto-html/popper.min.js"></script>
<script src="Type M error correction_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Type M error correction_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Type M error correction_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Type M error correction_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Type M error correction_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Type M error correction_files/libs/bootstrap/bootstrap-accd75c2bd871f03c4c02dcc9260e8c4.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


<link rel="stylesheet" href="_extensions/wjschne/apaquarto/apa.css">
<link rel="stylesheet" href="no-links.css">
</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content" id="quarto-document-content">




<br>

<br>

<section id="title" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Rethinking Type S and M Errors</h1>
<div class="Author">
<br>

<p>Daniel Lakens<sup>1</sup>, Cristian Mesquida<sup>1</sup>, Gabriela Xavier-Quintais<sup>2</sup>, Sajedeh Rasti<sup>1</sup>, Enrico Toffalini<sup>3</sup>, and Gianmarco Altoè<sup>4</sup></p>
<p><sup>1</sup>Industrial Engineering and Innovation Sciences, Eindhoven University of Technology</p>
<p><sup>2</sup>Faculty of Sciences, University of Lisbon</p>
<p><sup>3</sup>Department of General Psychology, University of Padova</p>
<p><sup>4</sup>Department of Developmental Psychology and Socialisation, University of Padova</p>
</div>
<br>

<br>

</section>
<section id="author-note" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Author Note</h1>
<p>Daniel Lakens <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0000-0002-8393-5316">https://orcid.org/0000-0002-8393-5316</a></p>
<p>Cristian Mesquida <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0000-0002-1542-8355">https://orcid.org/0000-0002-1542-8355</a></p>
<p>Gabriela Xavier-Quintais <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0000-000X-XXXX-XXXX">https://orcid.org/0000-000X-XXXX-XXXX</a></p>
<p>Sajedeh Rasti <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0009-0007-3416-7692">https://orcid.org/0009-0007-3416-7692</a></p>
<p>Enrico Toffalini <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0000-0002-1404-5133">https://orcid.org/0000-0002-1404-5133</a></p>
<p>Gianmarco Altoè <img src="_extensions/wjschne/apaquarto/ORCID-iD_icon-vector.svg" id="orchid" class="img-fluid" style="width:4.23mm" alt="Orcid ID Logo: A green circle with white letters ID"> <a href="https://orcid.org/0000-0003-1154-9528">https://orcid.org/0000-0003-1154-9528</a></p>
<p>Correspondence concerning this article should be addressed to Daniel Lakens, Industrial Engineering and Innovation Sciences, Eindhoven University of Technology, Den Dolech 2, 5600 MB, Eindhoven, Eindhoven, Email: <a href="mailto:D.Lakens@tue.nl">D.Lakens@tue.nl</a></p>
<br>
</section>
<section id="abstract" class="level1 unnumbered unlisted AuthorNote">
<h1 class="unnumbered unlisted AuthorNote">Abstract</h1>
<div class="AbstractFirstParagraph">
<p>Gelman and Carlin (2014) introduced Type S (sign) and Type M (magnitude) errors to highlight the possibility that statistically significant results in published articles are misleading. While these concepts have been proposed for both when designing a study (prospective) and when evaluating results in a published paper (retroactive), we argue that these statistics do not facilitate the proper design of studies, nor the meaningful interpretation of results. The idea to remove effects of exactly 0 from hypothesis tests is conceptually fraught, and Type 1 error control for minimum effect tests provide a more coherent solution to the same problem. Type M errors are primarily a tool to warn against effect size inflation after selectively reporting significant results, but we argue that statistical indices such as the critical effect size or bias adjusted effect size are preferable approaches. We do believe that Type S and M errors can be valuable pedagogically as part of statistics education where the principles of error control are explained, and consequences of bad research practices are explained. Type S and M errors could be used in the discussion section of studies that fail to follow good research practices. Overall, we argue their use-cases are more limited than is currently recognized, and alternative solutions deserve greater attention.</p>
</div>
<p><em>Keywords</em>: Type M error, Type S error, Error Control, Bias Correction</p>
<br>
</section>
<section id="firstheader" class="level1 title unnumbered unlisted">
<h1 class="title unnumbered unlisted">Rethinking Type S and M Errors</h1>
<p>Neyman-Pearson hypothesis testing is a widespread approach to statistical inferences in the social sciences. As a consequence of its popularity, it is commonly criticized, and critics regularly propose alternative statistical procedures to interpret results. One criticism of null hypothesis tests is that the null hypothesis is never true <span class="citation" data-cites="cohen1994">(<a href="#ref-cohen1994" role="doc-biblioref">Cohen, 1994</a>)</span>. Gelman and Tuerlinckx <span class="citation" data-cites="gelman2000">(<a href="#ref-gelman2000" role="doc-biblioref">2000</a>)</span> argue that because it seems unlikely that effect sizes are ever exactly zero for continuous data, it is uninteresting to control the probability of a Type 1 error. After all, if the null is never true, researchers do not need to worry about incorrectly claiming there is an effect, when there is no effect. They propose to compute the Type S error, or ‘sign error’, which quantifies the long-run frequency that a significant effect in the opposite direction of the true effect size is observed. They argue that Type S error rates are “the relevant error rate for statistical analyses in the social and behavioral sciences” (p.&nbsp;388). Gelman and Tuerlinckx also propose to report the Type M error, which quantifies the absolute average effect size inflation for a study design if only statistically significant results are reported. Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span> suggest that Type M errors are useful to understand that significant effect sizes from underpowered studies are almost certain to be a huge overestimate of the true effect. Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014, p. 644</a>)</span> say that problems with Type S errors become a concern “when power is less than 0.1” and Type M errors become a concern “when power is less than 0.5”.</p>
<p>Before criticizing the proposed use of Type S and M errors, it is important to acknowledge several points of agreement. First, we agree with Gelman and Tuerlinckx <span class="citation" data-cites="gelman2000">(<a href="#ref-gelman2000" role="doc-biblioref">2000</a>)</span> that it is uninteresting to test against a null hypothesis of no effect when it is highly unlikely that the true effect size is zero. However, we believe this problem should be addressed by performing a minimal effect test for superiority against a range of values that is considered practically equivalent to zero. Such tests against a non-nill null hypothesis have been suggested for more than half a century <span class="citation" data-cites="hodges1954 lakens_practical_2021 serlin_rationality_1985">Nunnally (<a href="#ref-nunnally1960" role="doc-biblioref">1960</a>)</span>. We also agree with Gelman and Carlin that performing an a priori power analysis based on estimates of a previous study or an expected effect size is not best practice, and leads to bias <span class="citation" data-cites="albers_whenpoweranalyses_2018">(<a href="#ref-albers_whenpoweranalyses_2018" role="doc-biblioref">Albers &amp; Lakens, 2018</a>)</span>. However, we see no problem with what is currently considered best practice - performing an a-priori based on a smallest effect size of interest (SESOI) - just because the true effect size might be smaller than the SESOI. Trivially small effect sizes are scientifically uninteresting, and researchers should not be interested in detecting effects that are too small to matter <span class="citation" data-cites="lakens_sample_2022">(<a href="#ref-lakens_sample_2022" role="doc-biblioref">Lakens, 2022</a>)</span>.</p>
<p>Third, we agree that publication bias is a systemic problem in the social sciences, which leads to inflated effect sizes in the scientific literature. However, we consider Type M errors less useful to deal with this problem than reporting the critical effect size <span class="citation" data-cites="perugini_benefits_2025">(e.g., <a href="#ref-perugini_benefits_2025" role="doc-biblioref">Perugini et al., 2025</a>)</span>. Finally, we recognize that it is valuable to retrospectively evaluate how informative studies in a literature are, especially in fields where good research practices are uncommon. However, we believe a sensitivity power analysis combined with bias correction methods such as p uniform <span class="citation" data-cites="aert_correcting_2018">(<a href="#ref-aert_correcting_2018" role="doc-biblioref">Aert &amp; Assen, 2018</a>)</span> or a maximum likelihood estimate for a truncated distribution <span class="citation" data-cites="anderson_addressing_2017 taylor_bias_1996">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">Anderson &amp; Maxwell, 2017</a>; <a href="#ref-taylor_bias_1996" role="doc-biblioref">Taylor &amp; Muller, 1996</a>)</span> are more insightful. Finally, the last two authors have co-authored scientific articles promoting Type S and M errors <span class="citation" data-cites="bertoldo_designing_2022 altoe_enhancing_2020">(<a href="#ref-altoe_enhancing_2020" role="doc-biblioref">Altoè et al., 2020</a>; <a href="#ref-bertoldo_designing_2022" role="doc-biblioref">Bertoldo et al., 2022</a>)</span>, and co-created the R package PRDA that computes Type S and M errors for different designs <span class="citation" data-cites="callegher_prda_2021">(<a href="#ref-callegher_prda_2021" role="doc-biblioref">Callegher et al., 2021</a>)</span>. They were invited to co-author this article to guarantee fair and nuanced arguments by incorporating internal criticism into this project <span class="citation" data-cites="lakens_pandemic_2020">(<a href="#ref-lakens_pandemic_2020" role="doc-biblioref">Lakens, 2020</a>)</span>.</p>
<section id="type-1-type-2-type-3-and-type-s-errors" class="level3">
<h3 data-anchor-id="type-1-type-2-type-3-and-type-s-errors"><strong>Type 1, Type 2, Type 3, and Type S errors</strong></h3>
<p>Type 1 errors occur when the null hypothesis is true, but a statistically significant result is observed. The null hypothesis in a two-sided test is that the population effect size is 0. The alternative hypothesis is that there is an effect - either in the positive or negative direction. After observing a statistically significant result the correct claim is that the null hypothesis can be rejected, and we can act - with an error rate of α - that there is a non-zero effect. Because no directional claims are made in a correctly performed two-sided null hypothesis test, sign errors are impossible in two-sided hypothesis tests. Sign errors only exist in directional tests based on distributions that have positive and negative values. This means Type S errors do not exist for distributions based on squared deviations or absolute values, such as when an omnibus F-test or Chi-squared test is performed to test if any difference exists.</p>
<p>Researchers sometimes incorrectly make a directional claim after a two-sided test, which can lead to what Kaiser <span class="citation" data-cites="kaiser_directional_1960">(<a href="#ref-kaiser_directional_1960" role="doc-biblioref">1960</a>)</span> has referred to as a Type 3 error. Type 3 errors occur when H0 is false, a statistically significant effect is observed in the opposite direction of the true effect, and researchers make a directional claim. The correct way to make directional claims is to perform a one-sided test. If researchers want to make a claim about an effect in the positive or negative direction, they should follow Kaiser’s <span class="citation" data-cites="kaiser_directional_1960">(<a href="#ref-kaiser_directional_1960" role="doc-biblioref">1960</a>)</span> proposal to perform two one-sided null-hypothesis tests, one in the positive direction, and one in the negative direction, each at α/2 <span class="citation" data-cites="leventhal_directional_1996">(see also <a href="#ref-leventhal_directional_1996" role="doc-biblioref">Leventhal &amp; Huynh, 1996</a>)</span>.&nbsp;</p>
<p>According to Gelman and Tuerlinckx <span class="citation" data-cites="gelman2000">(<a href="#ref-gelman2000" role="doc-biblioref">2000</a>)</span> effects are never exactly zero. This leads them to argue that it is uninteresting to perform a null-hypothesis test, or to control the Type 1 error rate. They propose to remove effects of 0 from the possible effect sizes that can be observed. This creates a situation where all effects are true effects, either in the positive or the negative direction. A statistical test now has two possible results:&nbsp; Effects in the opposite direction as the true effect size are correctly rejected, or effects in the true direction are incorrectly rejected - a Type S error. Type S errors can be computed for any test that makes claims based on a threshold <span class="citation" data-cites="lu_note_2019">(for closed form formula, see <a href="#ref-lu_note_2019" role="doc-biblioref">Lu et al., 2019</a>)</span>, but in this article we will focus on frequentist hypothesis tests. We set the alpha level for the test to 0.05 in all examples below.&nbsp;</p>
<p>If one believes effect sizes of 0 do not exist, it is no longer possible to perform a two-sided nill-null hypothesis test. Gelman and Tuerlinckx <span class="citation" data-cites="gelman2000">(<a href="#ref-gelman2000" role="doc-biblioref">2000</a>)</span> instead recommend to perform two simultaneous one-sided tests: One test in the positive direction, and one test in the negative direction. To understand Type S errors, it is useful to consider the most extreme scenario where the effect size is not zero. If there is an infinitesimally small positive effect, the statistical power of the test is practically indistinguishable from the alpha level of 0.05. Effects will be rejected in the positive direction with a probability of 0.025 in the long run (correct rejections), and effects will be rejected in the negative direction with a probability of 0.025 in the long run (Type S errors). The only reason we do not call a Type S error a Type 1 error is because Gelman and Tuerlinckx have removed the value of 0 from the distribution of possible effect sizes. A Type 1 error consists of all significant test results for values ≤ 0, while a Type S error consists of all significant test values &lt; 0. As any point in a continuous distribution has 0 probability, excluding the value of 0 will not change the long-run probabilities. Therefore, a Type S error has the same probability as a Type 1 error in a one-sided test. The difference between a Type 1 error and Type S error is purely conceptual, as Type 1 and Type S error rates are identical in one-sided tests.</p>
<p>When two one-sided tests are performed and the null is never true, the Type S error rate is at most α/2. Gelman and Turelinckx prefer to express the Type S error rate as a proportion of the significant results and “believe that this conditional probability is the appropriate error rate to consider, since our primary concern is to understand the frequency properties of claims with confidence”. In the infinitesimal effect case the statistical power of the test is 0.05 and the Type S error is 0.025, so the rate of Type S errors as a proportion of significant results is 0.025/0.05 = 0.5. In the extreme case where power is as low as the alpha level, 50% of significant results are sign errors.</p>
<p>This conditional probability is similar to the false discovery rate, which is the expected proportion of false positives among all positive findings. Just as Type 1 errors only occur when the null hypothesis is true, the false discovery rate only occurs when the null hypothesis is true. But once again, removing the value of 0, which itself has 0 probability, does not change the long-run false discovery rate. If researchers believe the null is true, a Type 1 error (a rejection of positive effects, when there is a positive effect) will be observed with a probability of 0.025, and a correct rejection (a rejection of negative effects, when there is a positive effect) will occur with a probability of 0.025. From all positive findings (0.025 false positives + 0.025 true positives = 0.05 positive test results) 50% are false positives. Therefore, the false discovery rate is 50% in the two one-sided testing procedure. The conditional Type S probability is identical to the false discovery rate, with the conceptual difference that it excludes the value of 0 from the hypothesized values.</p>
</section>
<section id="should-effects-of-0-be-considered-impossible" class="level3">
<h3 data-anchor-id="should-effects-of-0-be-considered-impossible"><strong>Should effects of 0 be considered impossible?</strong></h3>
<p>Note that it is somewhat peculiar to completely exclude the possibility that effect sizes of 0 exist in a hypothesis test. After all, one could just as easily argue that in continuous data, no true effect size is exactly 0.5, but no one would propose to remove 0.5 from the space of hypothesized values. Furthermore, as Type S errors do not exist for omnibus F-tests, Chi-squared tests, and other tests based on distributions that are bounded below by zero, the idea that an effect is never 0 implies there would never be any need to perform an omnibus F-test or Chi-squared test.</p>
<p>Researchers have also disagreed with the idea that effect sizes are never zero, with Krueger and Heck <span class="citation" data-cites="krueger_putting_2019">(<a href="#ref-krueger_putting_2019" role="doc-biblioref">2019</a>)</span> stating that: “for many questions humans ask of nature, the null (or any particular tested hypothesis) may in fact be true.” Frick <span class="citation" data-cites="frick_accepting_1995">(<a href="#ref-frick_accepting_1995" role="doc-biblioref">1995</a>)</span> concludes that “for some experiments, the null hypothesis is possible”. And Hagen <span class="citation" data-cites="hagen_praise_1997">(<a href="#ref-hagen_praise_1997" role="doc-biblioref">1997</a>)</span> drives home the criticism even more forcefully: “If, as some have claimed, the null hypothesis is always false, we would be foolish, indeed, to spend time conducting statistical tests that can only tell us what we already know. But we need not feel foolish. As far as I can tell, the claim has never been sustained by either statistical or logical arguments”. Neither side can provide empirical support for or against the hypothesis that the null hypothesis is never true, as we can’t measure the entire population for all effects scientists want to study. The claim that the null is never true is scientifically unfalsifiable.</p>
<p>It seems difficult to justify why one would believe the null is never true, while there is no doubt that an effect size of 1×10⁻³² can be true. Instead of discussing which effect sizes can be true, a more sensible concern is the idea that all variables are connected through theoretically uninteresting causal structures that result in non-zero correlations between variables (especially in observational studies). In psychology, this idea is referred to as the crud factor <span class="citation" data-cites="meehl_appraising_1990 orben_crud_2020">(<a href="#ref-meehl_appraising_1990" role="doc-biblioref">Meehl, 1990</a>; <a href="#ref-orben_crud_2020" role="doc-biblioref">Orben &amp; Lakens, 2020</a>)</span>. The crud factor has been one argument to move beyond null hypothesis significance tests, and instead test against a range of theoretically or practically uninteresting effect sizes around zero by performing a minimum effect test, or a test for superiority <span class="citation" data-cites="lakens_equivalence_2018">(<a href="#ref-lakens_equivalence_2018" role="doc-biblioref">Lakens et al., 2018</a>)</span>.</p>
<p>Instead of performing a test that rejects effects effect sizes of 0, researchers can specify a smallest effect size of interest, and test whether they can statistically reject all effects that are deemed too small to matter. Such a minimum effect tests resolve most of the problems researchers have with null-hypothesis significance testing <span class="citation" data-cites="lakens_practical_2021">(<a href="#ref-lakens_practical_2021" role="doc-biblioref">Lakens, 2021</a>)</span>, including the concern that the null is never true. If researchers do not believe it is scientifically interesting to reject effects of exactly 0 they can test whether effects in a range around 0 can be rejected. For example, Ferguson and Heene <span class="citation" data-cites="ferguson_providing_2021">(<a href="#ref-ferguson_providing_2021" role="doc-biblioref">2021</a>)</span> empirically show that correlational effects in aggression research are unlikely to ever be exactly 0, and established a correlation of <em>r</em> = 0.1 are a lower bound for hypothesis tests in this research field. A directional minimum effect test against <em>r</em> ≤ 0.1 is rejected if the lower bound of the 90% confidence interval around the observed correlation is larger than 0.1 <span class="citation" data-cites="lakens_equivalence_2018">(<a href="#ref-lakens_equivalence_2018" role="doc-biblioref">Lakens et al., 2018</a>)</span>. Minimum effect tests are a better solution than arbitrarily excluding a point value of 0 from effects that are possible. If minimum effect tests are adopted, the idea of a Type S error is no longer needed, as it is equivalent to a Type 1 error for one-sided tests against a non-nill null hypothesis.</p>
</section>
<section id="should-researchers-report-type-s-errors-in-scientific-articles" class="level3">
<h3 data-anchor-id="should-researchers-report-type-s-errors-in-scientific-articles"><strong>Should researchers report Type S errors in scientific articles?</strong></h3>
<p>Whereas Gelman and Tuerlinckx <span class="citation" data-cites="gelman2000">(<a href="#ref-gelman2000" role="doc-biblioref">2000</a>)</span> only discuss the relevance of Type S errors relative to Type 1 errors, Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span> take an additional step and suggest that it is useful to compute and report Type S error rates for specific studies. To compute a Type S error researchers need to specify what they believe is the true effect size. Gelman and Carlin suggest using a literature review or other available data. They provide an example where the original author observed an effect of 8 percentage points. Gelman and Carlin do not believe this finding can be correct, and retrospectively compute the Type S error based on effect sizes they believe are more likely to be true: 0.1, 0.3, and 1 percentage point. When they compute the Type S error for the much smaller effects they deem plausible, the probability of a sign error can be quite large as long as the statistical power is low.</p>
<p>Is it useful to compute and report Type S errors in this way? One problem with retrospective design analysis is that if a skeptic wants to argue that an effect size is unreliable, a retrospective design analysis will practically never prove them wrong. For any effect where a skeptical reader feels the need to question the scientific claim by performing a retrospective design analysis, there will be substantial uncertainty about the true effect size. In these cases, the literature (or any other external data) will rarely - if ever - provide strong constraints on how low effect sizes can plausibly be. This means a skeptical reader can always find reasons to perform a retrospective design analysis for very small effect sizes, which make it easy to claim that there is a high probability of a sign error. If the skeptic was wrong, a retrospective design analysis would practically never tell them they are wrong, and claims about high Type S error rates are therefore rarely severely tested.</p>
<p>To conclude this section, we have strong conceptual issues with Type S errors (removing 0 from the range of possible distributions is difficult to justify, and the long run probability of Type S errors is identical to the well-established Type 1 error rate and false discovery rate), as well as practical concerns (the test can too easily lead to a foregone conclusion). Type S errors mainly point out how uninformative studies with low statistical power are, and a null hypothesis test might not be an interesting question to ask.If researchers want to address these concerns, a better solution is to specify a non-null nill hypothesis (e.g., considering all effects between -0.1 and 0.1 as practically equivalent to 0) and perform an a-priori power analysis for the smallest effect size of interest. After the data is in, the Type S error is at most 2.5%, as is the Type 1 error. Researchers should always make a claim while acknowledging the maximum possible Type 1 error rate (e.g., 2.5% in an one-sided test with an alpha level of 0.025) and any quantification of how much lower the Type S error might be, depending on weakly informed guesses of the true effect size, comes with great uncertainty.</p>
<p>Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span> argue that a retrospective design analysis can reveal that ‘a study was too small to be informative’. But this seems misguided. If a significant effect was observed in a well-performed test we can reject the null with a maximum Type 1 error rate of 2.5%, which is small enough to take the result seriously. The study might have had very low power to detect an effect, but if it happened to detect an effect with the desired maximum Type 1 error rate, the fact that the study had a low probability to yield an informative outcome a-priori no longer matters. The observed effect size might be measured inaccurately, and inflated due to selection bias (see Type M errors below) but a researcher can claim that there is a very small probability that random noise is mistaken as a true effect.</p>
<p>The Type S error rate will only raise a red flag for studies with incredibly low statistical power. When the Type S error is high, the design is uninformative, but when the Type S error is low, the design is often still uninformative. The Type S error drops to negligible levels when the statistical power of the test is still unacceptably low. For example, Simonsohn <span class="citation" data-cites="simonsohn_small_2015">(<a href="#ref-simonsohn_small_2015" role="doc-biblioref">2015</a>)</span> considers a test with less than 33% power severely underpowered, but with 33% power the probability that a significant result is in the wrong direction is only 0.1%. A sensitivity power analysis provides more insight into how informative a test is, and is therefore a better approach than a retrospective design analysis.</p>
</section>
<section id="effect-size-inflation-and-type-m-errors" class="level3">
<h3 data-anchor-id="effect-size-inflation-and-type-m-errors"><strong>Effect size inflation and Type M errors</strong></h3>
<p>Lane and Dunlap <span class="citation" data-cites="lane_estimating_1978">(<a href="#ref-lane_estimating_1978" role="doc-biblioref">1978</a>)</span> examined the impact of selection bias on the inflation of effect size estimates. When only statistically significant results are shared, the effect size estimate based on these studies is inflated, as smaller non-significant effects are removed from the scientific literature. In general, how much effect sizes are inflated in the presence of selection bias is a function of the power of the test. The lower the statistical power - for example because population effect sizes or sample sizes are smaller - the more significant effect sizes will be inflated. <a href="#fig-effectsize-distribution" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a> shows the expected effect size distribution for an alternative hypothesis with a true effect size of Cohen’s <em>d</em> of 0.4. With a sample size of 40 participants in each group, a two-sided independent <em>t</em>-test will yield a statistically significant result if the observed effect size is larger than <em>d</em> = 0.445. If there is extreme selection bias for statistically significant results the scientific literature would consist of effect sizes to the right of <em>d</em> = 0.445. Instead of having access to the full distribution of expected effects, the literature will represent a truncated distribution, where all values smaller than 0.445 are removed from the distribution of expected effect sizes.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-effectsize-distribution" class="quarto-float quarto-figure quarto-figure-center" data-fig-align="center" prefix="" data-fignum="1">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-effectsize-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;1</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Expected effect size distribution under the alternative hypothesis, assuming a true effect size of 0.5 (dashed central line). With a sample size of 20 participants per group, a two-sided independent t-test yields a statistically significant result when the observed effect size exceeds d = 0.64. The shaded area represents the subset of effect sizes published under selection bias.</p>
</div>
</figcaption>
<div aria-describedby="fig-effectsize-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Type-M-error-correction_files/figure-html/fig-effectsize-distribution-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>This type of bias has been discussed extensively in the literature <span class="citation" data-cites="anderson_addressing_2017 begg_operating_1994 hedges_estimation_1984">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">Anderson &amp; Maxwell, 2017</a>; <a href="#ref-begg_operating_1994" role="doc-biblioref">Begg &amp; Mazumdar, 1994</a>; <a href="#ref-hedges_estimation_1984" role="doc-biblioref">Hedges, 1984</a>)</span>. Both Hedges <span class="citation" data-cites="hedges_estimation_1984">(<a href="#ref-hedges_estimation_1984" role="doc-biblioref">Hedges, 1984</a>)</span> as Taylor and Muller <span class="citation" data-cites="taylor_bias_1996">(<a href="#ref-taylor_bias_1996" role="doc-biblioref">Taylor &amp; Muller, 1996</a>)</span> have developed a statistical model that computes bias-adjusted effect size estimates when estimators are selected from a ‘censored’ or ‘truncated’ distribution. This method creates a model of what the full distribution would have looked like, and estimates the effect size if there was no selection bias. Anderson, Kelley, and Maxwell <span class="citation" data-cites="anderson_addressing_2017">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">2017</a>)</span> apply this method to compute an adjusted estimate of the noncentrality parameter, and use it to show how a-priori power analysis can be adjusted for selection bias.</p>
</div>
<p>Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span> propose another approach to increase awareness of effect size inflation due to selection bias. They compute a ratio between the average absolute effect size estimates from statistically significant effects, and the assumed true effect size, which they refer to as a Type M (magnitude) error. Because Type M errors do not quantify error rates (i.e., the probability of an incorrect claim) but the bias in an estimator, we prefer Gelman and Carlin’s alternative term for Type M errors: the ‘exaggeration ratio’. The exaggeration ratio is a function of the statistical power of a test. When power is close to 100% all tests of true effects will be significant, no effect sizes are removed due to selection bias, and the exaggeration ratio will be 1, indicating that the design will in the long run yield unbiased effect size estimates. As power is practically always less than 100%, the exaggeration ratio will typically be larger than 1, indicating that effect sizes selected for statistical significance will be inflated. Altoè et al. <span class="citation" data-cites="altoe_enhancing_2020">(<a href="#ref-altoe_enhancing_2020" role="doc-biblioref">2020</a>)</span> note that when power reaches at least 80% the average overestimation of the effect size tends to be just above 10%, which they consider practically negligible. As such, one may see the exaggeration ratio as just another way to interpret the same information conveyed by statistical power, with a focus shifted more on effect size estimation than on the consideration of the Type 2 error rate.</p>
<p>The exaggeration ratio is computed based on properties of the design of the study, and the assumed true effect size. The design (e.g., the sample size, type of test, and the alpha level) determine the critical effect size, or the smallest effect size that can be statistically significant. The assumed true effect size determines how inflated the average effect size in statistically significant results will be on average, with the smaller the assumed effect size, the larger the exaggeration ratio. The exaggeration ratio <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">Gelman &amp; Carlin, 2014</a>)</span> is not intended to correct individual effect sizes. If the average absolute unbiased effect size estimate is 0.5, and the average biased effect size estimate is 1, the estimated effect is on average 2 times larger than the true effect size. But this exaggeration ratio of 2 applies to the average effect size estimate, and not to any individual effect size. It is not correct to simply divide each observed effect size estimate by the exaggeration ratio and treat it as a bias-adjusted effect size estimate.</p>
<p>Despite the fact that the authors never intended the exaggeration ratio to be used to adjust individual effect sizes, researchers have misused Type M errors for this purpose. In the following example, Shem-Tov et al. <span class="citation" data-cites="shem-tov_can_2024">(<a href="#ref-shem-tov_can_2024" role="doc-biblioref">2024</a>)</span> note that the Type M error rate quantifies an average inflation, but still misuse it to adjust the observed effect size: “following the procedure proposed by Gelman and Carlin (2014), we estimate an average potential exaggeration ratio of 1.2 in the effect of enrollment to MIR on rearrests within one and four years. In other words, on average, our estimates might indicate that the impact of enrollment to MIR causes a reduction of 23.4 percentage points while the true effect is a reduction of 19.5 percentage points.” Similarly, Gajendran et al., <span class="citation" data-cites="gajendran_hidden_2022">(<a href="#ref-gajendran_hidden_2022" role="doc-biblioref">2022</a>)</span> write: “the modest Type M exaggeration ratio of 1.27 indicates the possibility that the communication medium effect is overestimated by a factor of 1.27, which is inconsistent with the effect being an unlikely result.” However, the average inflation is not the same as the inflation in any individual study, and the inflation might be much larger or smaller for this specific study.</p>
<p>There are statistical approaches that adjust effect size estimates for selection bias, such as likelihood based approaches <span class="citation" data-cites="hedges_estimation_1984 taylor_bias_1996">(<a href="#ref-hedges_estimation_1984" role="doc-biblioref">Hedges, 1984</a>; <a href="#ref-taylor_bias_1996" role="doc-biblioref">Taylor &amp; Muller, 1996</a>)</span> and <em>p</em>-uniform <span class="citation" data-cites="vanassen_metaanalysis_2015">(<a href="#ref-vanassen_metaanalysis_2015" role="doc-biblioref">Assen et al., 2015</a>)</span>. The approaches developed by Hedges <span class="citation" data-cites="hedges_estimation_1984">(<a href="#ref-hedges_estimation_1984" role="doc-biblioref">1984</a>)</span> and Taylor and Muller <span class="citation" data-cites="taylor_bias_1996">(<a href="#ref-taylor_bias_1996" role="doc-biblioref">Taylor &amp; Muller, 1996</a>)</span> to adjust effect size estimates for selection bias takes the observed effect size as a biased estimator, and based on a model for selection bias, computes the maximum likelihood estimate for the effect size after correcting for bias. Anderson and colleagues <span class="citation" data-cites="anderson_addressing_2017">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">2017</a>)</span> have implemented this bias correction method in the BUCSS R package, and extended the approach for normally distributed data to other statistical distributions. Although most other bias correction methods can only be used meta-analytically (i.e., they need multiple effect sizes to correct for bias) the p-uniform technique developed by Van Assen and colleagues <span class="citation" data-cites="vanassen_metaanalysis_2015">(<a href="#ref-vanassen_metaanalysis_2015" role="doc-biblioref">2015</a>)</span> uses a similar model for selection bias as Hedges <span class="citation" data-cites="hedges_estimation_1984">(<a href="#ref-hedges_estimation_1984" role="doc-biblioref">1984</a>)</span>, and can be performed on a single study. Note that BUCSS is aimed at sample size calculations, and does not compute bias-corrected effect sizes directly, but the non-centrality parameter that the function returns can be used to compute a bias-corrected effect size estimate. These approaches (as the simulations below will show) allow researchers to adjust observed effect sizes for bias corrected effect sizes.</p>
<p>In addition to bias-corrected effect size estimates, researchers can also report the critical effect size <span class="citation" data-cites="perugini_benefits_2025">(<a href="#ref-perugini_benefits_2025" role="doc-biblioref">Perugini et al., 2025</a>)</span>. The critical effect size is the smallest effect size that could reach statistical significance. Just as the Type M error, it is computed based on the study design, and not the study results. The critical effect size similarly increases awareness about how selection bias inflates effect size estimated from studies selected for statistical significance, but it also warns against interpreting non-significant results as the absence of an effect by pointing out the range of non-zero effect sizes that would never yield a statistically significant result. Critical effect sizes are also worth reporting in studies with very high statistical power, where the Type M error would not be reported because it is negligible. In studies with extremely high power, the critical effect size will make it clear that even trivially small effects will be statistically significant. This can make researchers reflect on which effects are large enough to be meaningful.</p>
</section>
<section id="simulating-effect-size-inflation-and-correction-methods" class="level3">
<h3 data-anchor-id="simulating-effect-size-inflation-and-correction-methods"><strong>Simulating effect size inflation and correction methods</strong></h3>
<p>We simulated 10.000 independent <em>t</em>-tests with a true effect size of <em>d</em> = 0.5, 40 observations per group, equal variances, and an alpha level of 0.05. We also computed adjusted effect size estimates using <em>p</em>-uniform and the bias and uncertainty-corrected sample size procedure (BUCSS). Note that the BUCSS R package returns a non-centrality parameter, which was transformed into a bias-adjusted effect size estimate. Van Aert et al. <span class="citation" data-cites="aert_publication_2019">(<a href="#ref-aert_publication_2019" role="doc-biblioref">2019, p. 16</a>)</span> note how the likelihood-based procedure implemented by Anderson and colleagues <span class="citation" data-cites="anderson_addressing_2017">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">2017</a>)</span> is ‘based on similar methodology’ as p-uniform. The bias and uncertainty-corrected sample size procedure (BUCSS) by Anderson and colleagues does not return an adjusted value if the effect size estimate is negative. P-uniform does return negative estimates. When applying p-uniform in its intended context of meta-analyses, the recommendation is to replace negative estimates by zero <span class="citation" data-cites="aert_publication_2019">(<a href="#ref-aert_publication_2019" role="doc-biblioref">Aert et al., 2019</a>)</span>. This solution is less ideal when analyzing single studies because too many estimates will be set to 0, which negatively biases the estimate. Instead, we follow Anderson and colleagues <span class="citation" data-cites="anderson_addressing_2017">(<a href="#ref-anderson_addressing_2017" role="doc-biblioref">2017</a>)</span> and remove negative estimates. Although we do not think this has been pointed out before in the literature, the corrections based on BUCSS and p-uniform are practically identical.</p>
<p>As noted above, the exaggeration ratio should not be used to adjust individual effect sizes, and was not developed for such a use. The distribution in the top-right pane of Figure 2 shows why this approach fails. Large observed effects are adjusted downward a lot, but smaller observed effect sizes are adjusted downward less, and not sufficiently. Where the average inflation can be used to educate researchers about the risk of inflation, it can’t be used to adjust individual effect sizes for selection bias.</p>
<p><a href="#fig-effectsize-distribution" class="quarto-xref" aria-expanded="false">Figure&nbsp;1</a> shows the effect size distribution of observed effect sizes when only statistically significant results are available (top-left pane). The total unbiased dataset would lead to a symmetrical distribution of observed effect sizes around 0.5. Given the sample size, only effects larger than <em>d</em> = 0.445 will be statistically significant, and all effects smaller than <em>d</em> = 0.445 are missing from the truncated distribution. The BUCSS and <em>p</em>-uniform methods lead to practically identical distributions of adjusted effect size estimates (bottom panes). The adjusted effect size estimates do not perfectly match the unbiased distribution, but provide estimates that are on average much closer to the true effect size. This demonstrated that it is more useful to report bias-adjusted effect size estimates using for example the p-uniform technique, than to report the Type M error - or even worse, misuse the exaggeration ratio to adjust observed effect sizes.</p>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output-display">
<div id="fig-simulation-distributions" class="quarto-float quarto-figure quarto-figure-center" data-fig-align="center" prefix="" data-fignum="2">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-simulation-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;2</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of the observed and bias-corrected effect size yielded by BUCSS, p-uniform, and the exaggeration ratio, and the statistical power of the test.</p>
</div>
</figcaption>
<div aria-describedby="fig-simulation-distributions-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Type-M-error-correction_files/figure-html/fig-simulation-distributions-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p><a href="#fig-simulation-distributions" class="quarto-xref" aria-expanded="false">Figure&nbsp;2</a> plots the observed Cohen’s <em>d</em> and bias-corrected Cohen’s <em>d</em> for independent t-tests with true effects from <em>d</em> = 0 to d = 1, 20 participants per group, and an alpha level of 0.05, with observed effect sizes on the y-axis, and corrected effect size estimates on the x-axis. The power of the test (which is a function of the effect size) is also plotted, and we see that when power is high (when Cohen’s <em>d</em> is large, e.g., from d = 0.75 onward) the observed Cohen’s <em>d</em> is practically equal to the corrected Cohen’s <em>d</em>. This is because there is very little selection bias, as most tests yield statistically significant results, and the observed effect size is not inflated. As the observed effect size becomes smaller, power is lower, selection bias is larger, and the corrected effect sizes become much smaller than the observed effect sizes for the BUCSS and <em>p</em>-uniform methods. We again see that these two methods yield basically the same corrected effect sizes. In this testing scenario the critical effect size is <em>d</em> = 0.445, which means effects smaller than this value will not yield a statistically significant result, and the correction methods only start to estimate positive effects from observed effect sizes of <em>d</em> = 0.52, as can be seen in the plot. Again, the yellow line visualizing a correction based on the exaggeration ratio illustrates why it should not be used to correct individual effects.</p>
</div>
<div class="cell FigureWithoutNote" data-layout-align="center" data-custom-style="FigureWithoutNote">
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.4451669</code></pre>
</div>
<div class="cell-output-display">
<div id="fig-correction-comparison" class="quarto-float quarto-figure quarto-figure-center" data-fig-align="center" prefix="" data-fignum="3">
<figure class="quarto-float quarto-float-fig figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-fig" id="fig-correction-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="FigureTitle" data-custom-style="FigureTitle">
<p>Figure&nbsp;3</p>
</div>
<div class="Caption" data-custom-style="Caption">
<p>Comparison of the corrected effect sizes for observed Cohen’s d effect sizes from 0 to 1 using BUCSS, p-uniform, and Type M errors.</p>
</div>
</figcaption>
<div aria-describedby="fig-correction-comparison-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="Type-M-error-correction_files/figure-html/fig-correction-comparison-1.svg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="624">
</div>
</figure>
</div>
</div>
</div>
<div class="AfterWithoutNote" data-custom-style="AfterWithoutNote">
<p>The exaggeration ratio assumes extreme selection bias where only statistically significant effects are available, which might be a reasonable expectation in some literatures <span class="citation" data-cites="scheel_excess_2021">(e.g., <a href="#ref-scheel_excess_2021" role="doc-biblioref">Scheel et al., 2021</a>)</span>, while there are more non-significant results published in other other literatures <span class="citation" data-cites="mesquida_publication_2023a">(e.g., <a href="#ref-mesquida_publication_2023a" role="doc-biblioref">Mesquida et al., 2023</a>)</span>. The only way to ensure unbiased effect size estimates is for researchers to report all results that are obtained, regardless of the significance level of test results. This can be achieved by reporting all studies in a public registry, or by publishing research as a Registered Report.</p>
</div>
</section>
<section id="the-use-of-type-s-and-m-error-in-an-imperfect-science" class="level3">
<h3 data-anchor-id="the-use-of-type-s-and-m-error-in-an-imperfect-science"><strong>The Use of Type S and M Error in an “Imperfect” Science</strong></h3>
<p>Although Type S and M errors might not be particularly informative to report in a result section, given the more informative alternatives (i.e., minimum effect tests and bias-corrected effect size estimates), they may still be valuable as a tool to reflect on the problems that emerge when studies are underpowered and/or selectively reported. Due to resource constraints, researchers may sometimes perform studies with very low power for the main effect of interest. Consider a PhD student who can at most collect 20 participants per group for an independent two-sided <em>t</em>-test, and expects a true effect size of <em>d</em> = 0.4. An a priori power analysis reveals that the statistical power is a mere 23%. With such a high probability of uninformative results, one might argue that the study should not be performed. However, the study is part of a grant proposal that funds the research of the PhD student, and their supervisor wants to complete the promised data collection, regardless of the low statistical power. Under these suboptimal conditions the student could reflect on the Type S and M error rates in a preregistration. They could point out that the effect size estimate has high uncertainty,&nbsp;that there is a probability of 0.5% that a statistically significant effect is in the wrong direction. Perhaps more informatively, the PhD includes the Type M error, warning readers that statistically significant effects will on average be inflated with a ratio of 2.06, meaning that any significant effect would be overestimated by 100% on average. This should make researchers aware of the fact that due to the underpowered nature of the study, the effect size estimate can not be taken at face value, especially if the effect size will only be interpreted if it is statistically significant.</p>
<p>A second use-case of a Type S and M error is in highly exploratory research where researchers opportunistically search for statistically significant effects without correcting for multiple comparisons. In such a scenario true effects might be small for some tests, researchers often selectively focus on statistically significant effects, and in the absence of strong theoretical predictions a skeptical peer might argue true effects might be small. Type S and M errors might be a way to communicate that exploratory analyses can be uninformative, and an exploratory search for significant effects will on average lead to exaggerated effect size estimates. This might help researchers realize that they should not make claims based on exploratory analyses as error rates can be high. Instead, results from exploratory analyses should be treated as hypotheses that need to be severely tested in follow-up studies <span class="citation" data-cites="ditroilo_exploratory_2025">(<a href="#ref-ditroilo_exploratory_2025" role="doc-biblioref">Ditroilo et al., 2025</a>)</span>.</p>
</section>
<section id="the-use-of-type-s-and-m-errors-in-statistics-education" class="level3">
<h3 data-anchor-id="the-use-of-type-s-and-m-errors-in-statistics-education"><strong>The Use of Type S and M Errors in Statistics Education</strong></h3>
<p>Statistical misconceptions are widespread among researchers. We believe that some statistical misconceptions that we have heard can be mitigated by educating researchers about Type S and M errors. For example, the statement ‘If the power of a study is low, the main risk is failing to detect an effect that is actually present’ overlooks the risk that a statistically significant effect from an underpowered study may be substantially overestimated. Another common misconception is that “If the sample is small and the result is statistically significant, the effect must be large”, which fails to acknowledge that the effect can be small, and that all effects selected for significance are substantially inflated. Type S and M errors can serve as effective educational tools to challenge such misconceptions and promote a deeper reflection on the risks involved in statistical inference under conditions of low power and selective reporting.</p>
<p>The idea of a Type S error can also help students grasp why it is incorrect to make directional claims after a two-sided test <span class="citation" data-cites="cho_twotailed_2013">(<a href="#ref-cho_twotailed_2013" role="doc-biblioref">Cho &amp; Abe, 2013</a>)</span>. The notion of a Type S error provides an opportunity to discuss why one-sided tests are necessary for directional hypotheses, and to introduce the practice of two one-sided tests at α/2 to test effects in both directions <span class="citation" data-cites="kaiser_directional_1960 leventhal_directional_1996">(<a href="#ref-kaiser_directional_1960" role="doc-biblioref">Kaiser, 1960</a>; <a href="#ref-leventhal_directional_1996" role="doc-biblioref">Leventhal &amp; Huynh, 1996</a>)</span>. Rather than teaching Type S errors as statistical quantities to compute and report in a manuscript, instructors can use them to emphasize the importance of aligning statistical decisions with the inferential goals. Instructors may also introduce the idea that when power is very low, even the direction of a statistically significant result can be misleading. This message may resonate more with students than visualizing power curves.</p>
<p>The concept of a Type M error—or exaggeration ratio—is especially helpful when introducing students to the consequences of publication bias and selective reporting. Even without delving into the mathematical derivation, Figure 1 clearly shows how filtering for significance results in inflated effect size estimates. This can serve as a foundation for teaching the idea that significant effects from underpowered studies are not only uncertain, but often systematically overestimated. The same idea can be taught through the concept of a critical effect size <span class="citation" data-cites="perugini_benefits_2025">(<a href="#ref-perugini_benefits_2025" role="doc-biblioref">Perugini et al., 2025</a>)</span> by illustrating how low power limits which effect sizes can be reliably distinguished from random noise. In follow-up courses, teachers could introduce the exaggeration ratio as a function of the statistical power of the test, and explain the limitations of underpowered studies. The Type M error can also be used to explain the difference between the inferential goals of hypothesis testing and estimation, revealing that tests that reject the null hypothesis inform us about the presence of an effect, but do not provide accurate effect size estimates. In more advanced courses, instructors may introduce tools for bias detection (e.g., funnel plots, p-curve analysis) or methods to compute bias-adjusted effect size estimates <span class="citation" data-cites="stanley_limitations_2017 simonsohn_p-curve_2014 vanassen_metaanalysis_2015 bartos_zcurve_2020">(<a href="#ref-vanassen_metaanalysis_2015" role="doc-biblioref">Assen et al., 2015</a>; <a href="#ref-bartos_zcurve_2020" role="doc-biblioref">Bartoš &amp; Schimmack, 2020</a>; <a href="#ref-simonsohn_p-curve_2014" role="doc-biblioref">Simonsohn et al., 2014</a>; <a href="#ref-stanley_limitations_2017" role="doc-biblioref">Stanley, 2017</a>)</span>.</p>
<p>When taught properly, the concepts of Type S and M errors can help to bridge the gap between statistical theory and the realities of scientific practice. They offer a narrative that aligns with the goals of open and rigorous science: understanding the risks of false directional claims, exploratory tests without error control, underpowered studies, and bias in the published literature, and emphasizing the value of reporting all research findings, for example through study registries or Registered Reports. Teaching students to think about the inferential claims they can—and cannot— make is important to improve their statistical literacy, and will bolster their critical thinking skills when they read claims in the scientific literature.&nbsp;</p>
</section>
<section id="conclusion" class="level3">
<h3 data-anchor-id="conclusion"><strong>Conclusion</strong></h3>
<p>Gelman and Carlin <span class="citation" data-cites="gelman2014">(<a href="#ref-gelman2014" role="doc-biblioref">2014</a>)</span> propose to compute Type S and M errors for planned or performed statistical tests (see also Altoè et al., 2020). However, both for conceptual and practical reasons we see limited value in quantifying Type S errors and the exaggeration ratio (or Type M error) for individual studies. We share the general concern that researchers should design informative studies, and be cautious when interpreting the results from underpowered studies, especially when combined with selection bias. Type S and Type M errors can be used to create awareness of the limitations of studies where researchers did not follow best practices, and they can play a role in statistics education to improve students’ understanding of how uninformative underpowered studies are. However, we believe there are more useful alternative statistical approaches to address these concerns. Instead of reporting Type S error rates, researchers should perform tests against a range of values considered theoretically or practically equivalent to 0. Instead of reporting Type M errors, researchers should report bias-corrected effect size estimates provided by methods such as p uniform, and report the critical effect size.</p>

</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-aert_correcting_2018" class="csl-entry" role="listitem">
Aert, R. C. M. van, &amp; Assen, M. A. L. M. van. (2018). <em>Correcting for publication bias in a meta-analysis with the p-uniform* method</em>. <a href="https://doi.org/10.31222/osf.io/zqjr9">https://doi.org/10.31222/osf.io/zqjr9</a>
</div>
<div id="ref-aert_publication_2019" class="csl-entry" role="listitem">
Aert, R. C. M. van, Wicherts, J. M., &amp; Assen, M. A. L. M. van. (2019). Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis. <em>PLOS ONE</em>, <em>14</em>(4), e0215052. <a href="https://doi.org/10.1371/journal.pone.0215052">https://doi.org/10.1371/journal.pone.0215052</a>
</div>
<div id="ref-albers_whenpoweranalyses_2018" class="csl-entry" role="listitem">
Albers, C. J., &amp; Lakens, D. (2018). When power analyses based on pilot data are biased: Inaccurate effect size estimators and follow-up bias. <em>Journal of Experimental Social Psychology</em>, <em>74</em>, 187–195. <a href="https://doi.org/10.1016/j.jesp.2017.09.004">https://doi.org/10.1016/j.jesp.2017.09.004</a>
</div>
<div id="ref-altoe_enhancing_2020" class="csl-entry" role="listitem">
Altoè, G., Bertoldo, G., Zandonella Callegher, C., Toffalini, E., Calcagnì, A., Finos, L., &amp; Pastore, M. (2020). Enhancing statistical inference in psychological research via prospective and retrospective design analysis. <em>Frontiers in Psychology</em>, <em>10</em>. <a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02893">https://www.frontiersin.org/articles/10.3389/fpsyg.2019.02893</a>
</div>
<div id="ref-anderson_addressing_2017" class="csl-entry" role="listitem">
Anderson, S. F., &amp; Maxwell, S. E. (2017). Addressing the <span>“replication crisis”</span>: Using original studies to design replication studies with appropriate statistical power. <em>Multivariate Behavioral Research</em>, 1–20. <a href="https://doi.org/10.1080/00273171.2017.1289361">https://doi.org/10.1080/00273171.2017.1289361</a>
</div>
<div id="ref-vanassen_metaanalysis_2015" class="csl-entry" role="listitem">
Assen, M. A. L. M. van, Aert, R. C. M. van, &amp; Wicherts, J. M. (2015). Meta-analysis using effect size distributions of only statistically significant studies. <em>Psychological Methods</em>, <em>20</em>(3), 293–309. <a href="https://doi.org/10.1037/met0000025">https://doi.org/10.1037/met0000025</a>
</div>
<div id="ref-bartos_zcurve_2020" class="csl-entry" role="listitem">
Bartoš, F., &amp; Schimmack, U. (2020). <em>Z-curve.2.0: Estimating replication rates and discovery rates</em>. <a href="https://doi.org/10.31234/osf.io/urgtn">https://doi.org/10.31234/osf.io/urgtn</a>
</div>
<div id="ref-begg_operating_1994" class="csl-entry" role="listitem">
Begg, C. B., &amp; Mazumdar, M. (1994). Operating characteristics of a rank correlation test for publication bias. <em>Biometrics</em>, <em>50</em>(4), 1088–1101.
</div>
<div id="ref-bertoldo_designing_2022" class="csl-entry" role="listitem">
Bertoldo, G., Callegher, C. Z., &amp; Altoè, G. (2022). Designing studies and evaluating research results: Type m and type s errors for pearson correlation coefficient. <em>Meta-Psychology</em>, <em>6</em>. <a href="https://doi.org/10.15626/MP.2020.2573">https://doi.org/10.15626/MP.2020.2573</a>
</div>
<div id="ref-callegher_prda_2021" class="csl-entry" role="listitem">
Callegher, C., Bertoldo, G., Toffalini, E., Vesely, A., Andreella, A., Pastore, M., &amp; Altoè, G. (2021). <em>PRDA: An r package for prospective and retrospective design analysis</em>. <a href="https://doi.org/10.21105/joss.02810">https://doi.org/10.21105/joss.02810</a>
</div>
<div id="ref-cho_twotailed_2013" class="csl-entry" role="listitem">
Cho, H.-C., &amp; Abe, S. (2013). Is two-tailed testing for directional research hypotheses tests legitimate? <em>Journal of Business Research</em>, <em>66</em>(9), 1261–1266. <a href="https://doi.org/10.1016/j.jbusres.2012.02.023">https://doi.org/10.1016/j.jbusres.2012.02.023</a>
</div>
<div id="ref-cohen1994" class="csl-entry" role="listitem">
Cohen, J. (1994). The earth is round (p &lt; .05). <em>American Psychologist</em>, <em>49</em>(12), 997–1003. <a href="https://doi.org/10.1037/0003-066X.49.12.997">https://doi.org/10.1037/0003-066X.49.12.997</a>
</div>
<div id="ref-ditroilo_exploratory_2025" class="csl-entry" role="listitem">
Ditroilo, M., Mesquida, Abt, &amp; Lakens, D. and. (2025). Exploratory research in sport and exercise science: Perceptions, challenges, and recommendations. <em>Journal of Sports Sciences</em>, <em>43</em>(12), 1108–1120. <a href="https://doi.org/10.1080/02640414.2025.2486871">https://doi.org/10.1080/02640414.2025.2486871</a>
</div>
<div id="ref-ferguson_providing_2021" class="csl-entry" role="listitem">
Ferguson, C. J., &amp; Heene, M. (2021). Providing a lower-bound estimate for psychology’s <span>“crud factor”</span>: The case of aggression. <em>Professional Psychology: Research and Practice</em>, <em>52</em>(6), 620–626. https://doi.org/<a href="https://doi.org/10.1037/pro0000386">https://doi.org/10.1037/pro0000386</a>
</div>
<div id="ref-frick_accepting_1995" class="csl-entry" role="listitem">
Frick, R. W. (1995). Accepting the null hypothesis. <em>Memory &amp; Cognition</em>, <em>23</em>(1), 132–138. <a href="https://doi.org/10.3758/BF03210562">https://doi.org/10.3758/BF03210562</a>
</div>
<div id="ref-gajendran_hidden_2022" class="csl-entry" role="listitem">
Gajendran, R. S., Loewenstein, J., Choi, H., &amp; Ozgen, S. (2022). Hidden costs of text-based electronic communication on complex reasoning tasks: Motivation maintenance and impaired downstream performance. <em>Organizational Behavior and Human Decision Processes</em>, <em>169</em>, 104130. <a href="https://doi.org/10.1016/j.obhdp.2022.104130">https://doi.org/10.1016/j.obhdp.2022.104130</a>
</div>
<div id="ref-gelman2014" class="csl-entry" role="listitem">
Gelman, A., &amp; Carlin, J. (2014). Beyond power calculations: Assessing type s (sign) and type m (magnitude) errors. <em>Perspectives on Psychological Science</em>, <em>9</em>(6), 641651. https://doi.org/<a href="https://doi.org/10.1177/1745691614551642">https://doi.org/10.1177/1745691614551642</a>
</div>
<div id="ref-gelman2000" class="csl-entry" role="listitem">
Gelman, A., &amp; Tuerlinckx, F. (2000). Type S error rates for classical and Bayesian single and multiple comparison procedures. <em>Computational Statistics</em>, <em>15</em>(3), 373–390. <a href="https://doi.org/10.1007/s001800000040">https://doi.org/10.1007/s001800000040</a>
</div>
<div id="ref-hagen_praise_1997" class="csl-entry" role="listitem">
Hagen, R. L. (1997). In praise of the null hypothesis statistical test. <em>The American Psychologist</em>, <em>52</em>(1), 15–24. <a href="http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=10561120">http://cat.inist.fr/?aModele=afficheN&amp;cpsidt=10561120</a>
</div>
<div id="ref-hedges_estimation_1984" class="csl-entry" role="listitem">
Hedges, L. V. (1984). Estimation of effect size under nonrandom sampling: The effects of censoring studies yielding statistically insignificant mean differences. <em>Journal of Educational Statistics</em>, <em>9</em>(1), 61–85. <a href="https://doi.org/10.3102/10769986009001061">https://doi.org/10.3102/10769986009001061</a>
</div>
<div id="ref-hodges1954" class="csl-entry" role="listitem">
Hodges, J. L., &amp; Lehmann, E. L. (1954). Testing the approximate validity of statistical hypotheses. <em>Journal of the Royal Statistical Society. Series B (Methodological)</em>, <em>16</em>(2), 261–268. https://doi.org/<a href="https://doi.org/10.1111/j.2517-6161.1954.tb00169.x">https://doi.org/10.1111/j.2517-6161.1954.tb00169.x</a>
</div>
<div id="ref-kaiser_directional_1960" class="csl-entry" role="listitem">
Kaiser, H. F. (1960). Directional statistical decisions. <em>Psychological Review</em>, <em>67</em>(3), 160–167. https://doi.org/<a href="https://doi.org/10.1037/h0047595">https://doi.org/10.1037/h0047595</a>
</div>
<div id="ref-krueger_putting_2019" class="csl-entry" role="listitem">
Krueger, J. I., &amp; Heck, P. R. (2019). Putting the p-value in its place. <em>The American Statistician</em>, <em>73</em>(sup1), 122–128. <a href="https://doi.org/10.1080/00031305.2018.1470033">https://doi.org/10.1080/00031305.2018.1470033</a>
</div>
<div id="ref-lakens_pandemic_2020" class="csl-entry" role="listitem">
Lakens, D. (2020). Pandemic researchers — recruit your own best critics. <em>Nature</em>, <em>581</em>(78077807), 121–121. <a href="https://doi.org/10.1038/d41586-020-01392-8">https://doi.org/10.1038/d41586-020-01392-8</a>
</div>
<div id="ref-lakens_practical_2021" class="csl-entry" role="listitem">
Lakens, D. (2021). The practical alternative to the p value is the correctly used p value. <em>Perspectives on Psychological Science</em>, <em>16</em>(3), 639–648. <a href="https://doi.org/10.1177/1745691620958012">https://doi.org/10.1177/1745691620958012</a>
</div>
<div id="ref-lakens_sample_2022" class="csl-entry" role="listitem">
Lakens, D. (2022). Sample size justification. <em>Collabra: Psychology</em>, <em>8</em>(1), 33267. <a href="https://doi.org/10.1525/collabra.33267">https://doi.org/10.1525/collabra.33267</a>
</div>
<div id="ref-lakens_equivalence_2018" class="csl-entry" role="listitem">
Lakens, D., Scheel, A. M., &amp; Isager, P. M. (2018). Equivalence testing for psychological research: A tutorial. <em>Advances in Methods and Practices in Psychological Science</em>, <em>1</em>(2), 259–269. <a href="https://doi.org/10.1177/2515245918770963">https://doi.org/10.1177/2515245918770963</a>
</div>
<div id="ref-lane_estimating_1978" class="csl-entry" role="listitem">
Lane, D. M., &amp; Dunlap, W. P. (1978). Estimating effect size: Bias resulting from the significance criterion in editorial decisions. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>31</em>(2), 107–112. <a href="https://doi.org/10.1111/j.2044-8317.1978.tb00578.x">https://doi.org/10.1111/j.2044-8317.1978.tb00578.x</a>
</div>
<div id="ref-leventhal_directional_1996" class="csl-entry" role="listitem">
Leventhal, L., &amp; Huynh, C.-L. (1996). Directional decisions for two-tailed tests: Power, error rates, and sample size. <em>Psychological Methods</em>, <em>1</em>(3), 278–292. https://doi.org/<a href="http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.1.3.278">http://dx.doi.org.dianus.libr.tue.nl/10.1037/1082-989X.1.3.278</a>
</div>
<div id="ref-lu_note_2019" class="csl-entry" role="listitem">
Lu, J., Qiu, Y., &amp; Deng, A. (2019). A note on type s/m errors in hypothesis testing. <em>British Journal of Mathematical and Statistical Psychology</em>, <em>72</em>(1), 1–17. <a href="https://doi.org/10.1111/bmsp.12132">https://doi.org/10.1111/bmsp.12132</a>
</div>
<div id="ref-meehl_appraising_1990" class="csl-entry" role="listitem">
Meehl, P. E. (1990). Appraising and amending theories: The strategy of lakatosian defense and two principles that warrant it. <em>Psychological Inquiry</em>, <em>1</em>(2), 108–141. <a href="https://doi.org/10.1207/s15327965pli0102_1">https://doi.org/10.1207/s15327965pli0102_1</a>
</div>
<div id="ref-mesquida_publication_2023a" class="csl-entry" role="listitem">
Mesquida, C., Murphy, J., Lakens, D., &amp; Warne, J. (2023). Publication bias, statistical power and reporting practices in the journal of sports sciences: Potential barriers to replicability. <em>Journal of Sports Sciences</em>, 1–11. <a href="https://doi.org/10.1080/02640414.2023.2269357">https://doi.org/10.1080/02640414.2023.2269357</a>
</div>
<div id="ref-nunnally1960" class="csl-entry" role="listitem">
Nunnally, J. (1960). The place of statistics in psychology. <em>Educational and Psychological Measurement</em>, <em>20</em>(4), 641650. https://doi.org/<a href="https://doi.org/10.1177/001316446002000401">https://doi.org/10.1177/001316446002000401</a>
</div>
<div id="ref-orben_crud_2020" class="csl-entry" role="listitem">
Orben, A., &amp; Lakens, D. (2020). Crud (re)defined. <em>Advances in Methods and Practices in Psychological Science</em>, <em>3</em>(2), 238–247. <a href="https://doi.org/10.1177/2515245920917961">https://doi.org/10.1177/2515245920917961</a>
</div>
<div id="ref-perugini_benefits_2025" class="csl-entry" role="listitem">
Perugini, A., Toffalini, E., Gambarota, F., Lakens, D., Pastore, M., Finos, L., &amp; Altoè, G. (2025). The benefits of reporting critical effect size values. <em>Advances in Methods and Practices in Psychological Science</em>. <a href="https://doi.org/10.31234/osf.io/7qe92">https://doi.org/10.31234/osf.io/7qe92</a>
</div>
<div id="ref-scheel_excess_2021" class="csl-entry" role="listitem">
Scheel, A. M., Schijen, M. R. M. J., &amp; Lakens, D. (2021). An excess of positive results: Comparing the standard psychology literature with registered reports. <em>Advances in Methods and Practices in Psychological Science</em>, <em>4</em>(2). <a href="https://doi.org/10.1177/25152459211007467">https://doi.org/10.1177/25152459211007467</a>
</div>
<div id="ref-serlin_rationality_1985" class="csl-entry" role="listitem">
Serlin, R. C., &amp; Lapsley, D. K. (1985). Rationality in psychological research: The good-enough principle. <em>American Psychologist</em>, <em>40</em>(1), 73–83. <a href="https://doi.org/10.1037/0003-066X.40.1.73">https://doi.org/10.1037/0003-066X.40.1.73</a>
</div>
<div id="ref-shem-tov_can_2024" class="csl-entry" role="listitem">
Shem-Tov, Y., Raphael, S., &amp; Skog, A. (2024). Can restorative justice conferencing reduce recidivism? Evidence from the make-it-right program. <em>Econometrica</em>, <em>92</em>(1), 61–78. <a href="https://doi.org/10.3982/ECTA20996">https://doi.org/10.3982/ECTA20996</a>
</div>
<div id="ref-simonsohn_small_2015" class="csl-entry" role="listitem">
Simonsohn, U. (2015). Small telescopes: Detectability and the evaluation of replication results. <em>Psychological Science</em>, <em>26</em>(5), 559–569. <a href="https://doi.org/10.1177/0956797614567341">https://doi.org/10.1177/0956797614567341</a>
</div>
<div id="ref-simonsohn_p-curve_2014" class="csl-entry" role="listitem">
Simonsohn, U., Nelson, L. D., &amp; Simmons, J. P. (2014). P-curve: A key to the file-drawer. <em>Journal of Experimental Psychology: General</em>, <em>143</em>(2), 534–547. https://doi.org/<a href="https://doi.org/10.1037/a0033242">https://doi.org/10.1037/a0033242</a>
</div>
<div id="ref-stanley_limitations_2017" class="csl-entry" role="listitem">
Stanley, T. D. (2017). Limitations of PET-PEESE and other meta-analysis methods. <em>Social Psychological and Personality Science</em>, 194855061769306. <a href="https://doi.org/10.1177/1948550617693062">https://doi.org/10.1177/1948550617693062</a>
</div>
<div id="ref-taylor_bias_1996" class="csl-entry" role="listitem">
Taylor, D. J., &amp; Muller, K. E. (1996). Bias in linear model power and sample size calculation due to estimating noncentrality. <em>Communications in Statistics-Theory and Methods</em>, <em>25</em>(7), 1595–1610. <a href="https://doi.org/10.1080/03610929608831787">https://doi.org/10.1080/03610929608831787</a>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>